# Кодировки

Очень важно различать два основных понятия: текст и его представление в виде байт.
Есть две основные операции перевода из одной сущности в другую: из текста в байты 
с помощью какой-то кодировки(обычно называется encode), и наоборот(decode).

Кодировка - способ отображения текста в байты и наоборот.

Одной из первых кодировок являлась ASCII. 
Она в соответствие каждому символу ставит семибитовый код(т.е. хранится 128 символов).
32 символа - управляющие, остальные - стандартные английские буквы, цифры, знаки препинания и т.д.

В какой-то момент основной единицей в компьютерах стал байт.
Т.е. теперь мы уже можем поместить в два раза больше символов.
Причём при разработке новой кодировки требовалось сохранить обратную совместимость
с базовой ASCII-частью, а вторую половину заполнить некоторым образом.
В разных странах эту проблему решали по-разному, что привело к появлению
большого количества различных однобайтовых кодировок.
Примерами таких кодировок являются например 
KOI-8(кириллическая кодировка, имеющая интересную особенность:
в случае, если используемый редактор не поддерживает эту кодировку, 
при преобразовании кириллических символов в ASCII получался транслит;
соответственно русские буквы шли не по алфавиту),
CP866(кодировка, используемая в MSDOS),
CP1251(дефолтная кодировка при использовании кириллицы в windows до windows 10).

Но понятно, что символов чуть больше, чем 256.
И наличие различных кодировок не является универсальным решением.
При попытке стандартизировать появился Unicode. 
Он также является отображением из кода в символ, только размеры чуть больше:
2^21 символов.

Unicode реализует несколько основных кодировок:
1. UTF-32. 
Каждый символ занимает ровно 4 байта.
С ней легче всего работать, но она занимает больше всего памяти из-за 
фиксированного размера.
2. UTF-16.
Каждый символ представлен либо 2, либо 4 байтами.
3. UTF-8.
Символ кодируется 1, 2, 3 или 4 байтами.
ASCII-часть кодируется одним байтом, причём коды символов полностью совпадают.
> Устройство UTF-8.
>
> Если говорить об ASCII-символе, то в начале идёт 0, а потом 7 бит символа.
>
> Если символ не ASCII, тогда следующим образом: сначала понимаем, сколько байт
> нужно для представления символа.
> В самом первом байте вместо нуля пишется некоторое количество единиц.
> Это число указывает на то, сколько байт занимает символ.
> Т.е. если он занимает 3 байта, то пишется 111, после чего пишется 0.
> Далее идут уже значащие для кода биты.
> Каждый последующий байт начинается с 10, после чего идут 6 значащих битов.
>
> Можно спросить, зачем нужна такая избыточность.
> При разработке кодировки требовалось сохранить обратную совместимость 
> с уже большой кодовой базой, написаной на C, в котором
> нулевой символ является знаком окончания строки.
> Как видим, нулевой символ в UTF-8 только один.
> Также становится понятно, что если очередной символ не начинается с 10, 
> то некоторое количество стоит пропустить, т.к. байты битые.
> Ещё тут выполняется такое свойство, что ни один символ не является префиксом
> другого, что позволяет использовать некоторый набор старых функций, работающих
> с ASCII с UTF-8(например поиск подстроки в строке).

### Литература

+ [The absolute minimum every software absolutely positively must 
know about unicode and character sets no excuses.](
https://www.joelonsoftware.com/2003/10/08/the-absolute-minimum-every-software-developer-absolutely-positively-must-know-about-unicode-and-character-sets-no-excuses/
).
